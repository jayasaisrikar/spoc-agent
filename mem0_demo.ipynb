{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c9a88d2",
   "metadata": {},
   "source": [
    "# Mem0 Integration with Codebase Agent\n",
    "\n",
    "This notebook demonstrates how to implement mem0 (long-term memory) with Google Gemini in your Codebase Agent project. Mem0 allows your AI assistant to remember previous conversations and provide context-aware responses.\n",
    "\n",
    "## Key Features\n",
    "- **Persistent Memory**: Conversations are stored and can be retrieved across sessions\n",
    "- **Semantic Search**: Find relevant memories based on context, not just keywords\n",
    "- **Enhanced Context**: AI responses become more personalized and contextually aware\n",
    "- **Repository-Specific Memory**: Different memory contexts for different code repositories\n",
    "\n",
    "Let's explore how to integrate this powerful memory system into your existing codebase analysis tool!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c35b95e",
   "metadata": {},
   "source": [
    "## 1. Setup: Install Required Packages\n",
    "\n",
    "First, we need to install the required packages for Google Gemini and mem0. Make sure you have a Google API key set in your environment variables as `GEMINI_API_KEY`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd5d0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install google-genai mem0ai --upgrade\n",
    "\n",
    "# Also install any missing dependencies\n",
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f537eb0c",
   "metadata": {},
   "source": [
    "## 2. Initialize Google Gemini and Mem0 Memory\n",
    "\n",
    "Now we'll set up the Google Gemini client and configure mem0 with Gemini as both the LLM and embedding model. We'll use a local Qdrant instance as the vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dd5bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google import genai\n",
    "from mem0 import Memory\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Create Gemini client\n",
    "client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "\n",
    "# Configure mem0 with Gemini models\n",
    "config = {\n",
    "    \"embedder\": {\n",
    "        \"provider\": \"gemini\",\n",
    "        \"config\": {\n",
    "            \"model\": \"models/text-embedding-004\",\n",
    "        }\n",
    "    },\n",
    "    \"llm\": {\n",
    "        \"provider\": \"gemini\",\n",
    "        \"config\": {\n",
    "            \"model\": \"gemini-2.5-flash\",\n",
    "            \"temperature\": 0.0,\n",
    "            \"max_tokens\": 2000,\n",
    "        }\n",
    "    },\n",
    "    \"vector_store\": {\n",
    "        \"config\": {\n",
    "            \"embedding_model_dims\": 768,\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize memory system\n",
    "memory = Memory.from_config(config)\n",
    "print(\"‚úÖ Memory system initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893f1ce1",
   "metadata": {},
   "source": [
    "## 3. Store User Memories from Example Conversations\n",
    "\n",
    "Let's simulate a developer working with our codebase agent by adding some example conversations to memory. These will demonstrate how the system remembers user preferences and project context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68998b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example conversation 1: Repository analysis preferences\n",
    "conv1 = [\n",
    "    {\"role\": \"user\", \"content\": \"I just uploaded my FastAPI project to analyze. I'm particularly interested in understanding the database architecture and API endpoint organization. I prefer detailed Mermaid diagrams that show the data flow.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"I've analyzed your FastAPI project. The database architecture uses SQLAlchemy with a clear separation between models, schemas, and database operations. Your API endpoints are well-organized with proper dependency injection. I've generated a detailed Mermaid diagram showing the data flow from HTTP requests through your router layers to the database. The diagram highlights how your authentication middleware intercepts requests and how your CRUD operations are structured.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Perfect! Can you also identify potential areas where I could add caching? I'm using Redis for session storage but wondering about query caching.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Based on your codebase structure, I see several opportunities for query caching. Your user lookup operations in the authentication service would benefit from Redis caching, especially the `get_user_by_email` function. I also notice your analytics endpoints perform complex aggregations that could be cached. I recommend adding a caching layer in your service classes, particularly for read-heavy operations like dashboard data retrieval.\"}\n",
    "]\n",
    "\n",
    "# Add to memory for user \"developer_sarah\"\n",
    "result1 = memory.add(conv1, user_id=\"developer_sarah\")\n",
    "print(\"‚úÖ Added conversation 1 about FastAPI project analysis\")\n",
    "\n",
    "# Example conversation 2: Feature implementation preferences  \n",
    "conv2 = [\n",
    "    {\"role\": \"user\", \"content\": \"I want to add a real-time notification system to my project. I prefer using WebSockets over Server-Sent Events, and I like to keep all real-time logic in separate modules for better organization.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Excellent choice! For your FastAPI project, I recommend implementing WebSockets using FastAPI's built-in support. Based on your preference for modular organization, I suggest creating a `realtime` package with separate modules: `websocket_manager.py` for connection handling, `notification_service.py` for business logic, and `events.py` for event definitions. This aligns with your existing project structure where you separate concerns clearly.\"},\n",
    "    {\"role\": \"user\", \"content\": \"That sounds perfect. I also want to make sure the notifications are persisted to the database even if users are offline. Should I use a message queue?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Given your SQLAlchemy setup, I recommend adding a `notifications` table to store persistent notifications. For the message queue, Redis (which you're already using) with rq or Celery would be ideal. This way you can queue notification tasks, store them in the database, and deliver them via WebSocket when users come online. The architecture would fit well with your existing database patterns.\"}\n",
    "]\n",
    "\n",
    "result2 = memory.add(conv2, user_id=\"developer_sarah\")\n",
    "print(\"‚úÖ Added conversation 2 about WebSocket implementation\")\n",
    "\n",
    "# Example conversation 3: Code quality preferences\n",
    "conv3 = [\n",
    "    {\"role\": \"user\", \"content\": \"I'm working on improving code quality in my repositories. I really value comprehensive type hints, detailed docstrings, and consistent error handling patterns. Can you analyze my latest commit and suggest improvements?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"I've analyzed your recent commits and I'm impressed with your typing discipline! Your error handling is quite good, though I notice some inconsistency in exception types. I recommend creating custom exception classes that inherit from HTTPException for API-specific errors. For docstrings, you're doing well with most functions, but some utility functions could use more detailed parameter descriptions. Your type hints are excellent - you're already using Union types and Optional properly.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Thanks! I do tend to rush the utility functions. What about testing? I'm trying to get better at writing comprehensive tests.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Your test structure is solid! I see you're using pytest with good fixture organization. One suggestion: your API tests could benefit from more edge case coverage, particularly around authentication and input validation. Consider adding tests for malformed requests and expired tokens. Also, your database tests are thorough, but adding some property-based tests using Hypothesis could help catch edge cases in your data validation logic.\"}\n",
    "]\n",
    "\n",
    "result3 = memory.add(conv3, user_id=\"developer_sarah\")  \n",
    "print(\"‚úÖ Added conversation 3 about code quality preferences\")\n",
    "\n",
    "print(f\"\\nüéâ Successfully stored {len([conv1, conv2, conv3])} conversations in memory!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ddff38",
   "metadata": {},
   "source": [
    "## 4. Search and Retrieve Memories\n",
    "\n",
    "Now let's see how we can search through the stored memories to find relevant information about the user's preferences and past interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a18051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for information about the user's preferences\n",
    "print(\"üîç Searching for user preferences...\")\n",
    "user_preferences = memory.search(query=\"What are the user's preferences?\", user_id=\"developer_sarah\")\n",
    "\n",
    "print(\"\\nüìã User Preferences Found:\")\n",
    "for i, memory_item in enumerate(user_preferences[\"results\"][:5], 1):\n",
    "    print(f\"{i}. {memory_item['memory']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Search for technical preferences\n",
    "print(\"\\nüîç Searching for technical implementation preferences...\")\n",
    "tech_preferences = memory.search(query=\"technical preferences implementation patterns\", user_id=\"developer_sarah\")\n",
    "\n",
    "print(\"\\nüõ†Ô∏è Technical Preferences:\")\n",
    "for i, memory_item in enumerate(tech_preferences[\"results\"][:3], 1):\n",
    "    print(f\"{i}. {memory_item['memory']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Search for past project analysis\n",
    "print(\"\\nüîç Searching for past project analysis...\")\n",
    "project_info = memory.search(query=\"FastAPI project database\", user_id=\"developer_sarah\")\n",
    "\n",
    "print(\"\\nüìä Project Analysis History:\")\n",
    "for i, memory_item in enumerate(project_info[\"results\"][:3], 1):\n",
    "    print(f\"{i}. {memory_item['memory']}\")\n",
    "\n",
    "print(f\"\\nüìà Total memories found: {len(user_preferences['results'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94660460",
   "metadata": {},
   "source": [
    "## 5. Compare Gemini Responses With and Without Memory\n",
    "\n",
    "Let's see the difference between Gemini's responses when it has no context versus when it has access to the user's memory and preferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f74354c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prompt about implementing a new feature\n",
    "test_prompt = \"I want to add logging to my application. Can you suggest the best approach and implementation strategy?\"\n",
    "\n",
    "print(\"ü§ñ Testing Gemini Response WITHOUT Memory Context\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Response without memory\n",
    "basic_system_prompt = \"You are a helpful AI assistant that helps with code architecture and implementation.\"\n",
    "\n",
    "response_no_memory = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=test_prompt,\n",
    "    config={\n",
    "        \"system_instruction\": basic_system_prompt\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Response without memory:\")\n",
    "print(response_no_memory.text)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üß† Testing Gemini Response WITH Memory Context\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Retrieve relevant memories for the prompt\n",
    "relevant_memories = memory.search(query=test_prompt, user_id=\"developer_sarah\")\n",
    "memories_str = \"\\n\".join(f\"- {entry['memory']}\" for entry in relevant_memories[\"results\"])\n",
    "\n",
    "# Enhanced system prompt with memory\n",
    "enhanced_system_prompt = f\"\"\"You are a helpful AI assistant that helps with code architecture and implementation.\n",
    "\n",
    "Based on previous conversations, here's what you know about the user:\n",
    "{memories_str}\n",
    "\n",
    "Use this context to provide personalized recommendations that align with their preferences and project structure.\"\"\"\n",
    "\n",
    "response_with_memory = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\", \n",
    "    contents=test_prompt,\n",
    "    config={\n",
    "        \"system_instruction\": enhanced_system_prompt\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Response with memory context:\")\n",
    "print(response_with_memory.text)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä COMPARISON SUMMARY:\")\n",
    "print(\"Without memory: Generic logging advice\")\n",
    "print(\"With memory: Personalized recommendations based on user's FastAPI project, modular preferences, and code quality standards\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70443d2c",
   "metadata": {},
   "source": [
    "## 6. Interactive Long-term Memory Chatbot\n",
    "\n",
    "Now let's create an interactive chatbot that maintains memory across conversations. This simulates how your codebase agent can remember user preferences and project context across multiple sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64c0477",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_memory_codebase_agent(history: list, user_id: str = \"developer_sarah\") -> list:\n",
    "    \"\"\"Enhanced chatbot with memory for codebase analysis\"\"\"\n",
    "    \n",
    "    # Get the latest user message\n",
    "    latest_message = history[-1][\"parts\"][0][\"text\"]\n",
    "    print(f\"üîç User: {latest_message}\")\n",
    "    \n",
    "    # Retrieve relevant memories\n",
    "    relevant_memories = memory.search(query=latest_message, user_id=user_id, limit=5)\n",
    "    memories_str = \"\\n\".join(f\"- {entry['memory']}\" for entry in relevant_memories[\"results\"])\n",
    "    \n",
    "    # Enhanced system prompt for codebase agent\n",
    "    memory_system_prompt = f\"\"\"You are an expert code architecture assistant with access to the user's conversation history and preferences.\n",
    "\n",
    "User Context and Preferences:\n",
    "{memories_str}\n",
    "\n",
    "Guidelines:\n",
    "- Use the user's past preferences to inform your recommendations\n",
    "- Reference their previous projects and patterns when relevant\n",
    "- Maintain consistency with their coding standards and architectural choices\n",
    "- Provide specific, actionable advice based on their tech stack and preferences\n",
    "- Remember their project structure and suggest improvements that align with their existing patterns\n",
    "\n",
    "Answer based on the conversation history and user preferences.\"\"\"\n",
    "    \n",
    "    # Generate response with memory context\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        contents=history,\n",
    "        config={\"system_instruction\": memory_system_prompt}\n",
    "    )\n",
    "    \n",
    "    # Add assistant response to history\n",
    "    history.append({\"role\": \"model\", \"parts\": [{\"text\": response.text}]})\n",
    "    \n",
    "    # Convert history to memory format and store\n",
    "    messages = []\n",
    "    for i, turn in enumerate(history):\n",
    "        role = \"user\" if turn[\"role\"] == \"user\" else \"assistant\"\n",
    "        content = turn[\"parts\"][0][\"text\"]\n",
    "        messages.append({\"role\": role, \"content\": content})\n",
    "    \n",
    "    # Add new conversation to memory\n",
    "    memory.add(messages, user_id=user_id)\n",
    "    \n",
    "    print(f\"ü§ñ Assistant: {response.text}\")\n",
    "    return history\n",
    "\n",
    "def run_codebase_agent_chat():\n",
    "    \"\"\"Interactive chat session with memory\"\"\"\n",
    "    print(\"üöÄ Codebase Agent with Memory - Type 'exit' to quit\")\n",
    "    print(\"üí° Try asking about: code architecture, implementing features, or improving code quality\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    history = []\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"\\nüí≠ You: \").strip()\n",
    "        \n",
    "        if user_input.lower() == 'exit':\n",
    "            print(\"üëã Goodbye! Your conversation has been saved to memory.\")\n",
    "            break\n",
    "            \n",
    "        # Add user message to history\n",
    "        history.append({\"role\": \"user\", \"parts\": [{\"text\": user_input}]})\n",
    "        \n",
    "        # Get response with memory\n",
    "        history = chat_with_memory_codebase_agent(history)\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "# Run the interactive chat\n",
    "# Uncomment the line below to start the interactive session\n",
    "# run_codebase_agent_chat()\n",
    "\n",
    "print(\"üéØ Interactive chatbot function created!\")\n",
    "print(\"üìù To start chatting, uncomment and run: run_codebase_agent_chat()\")\n",
    "print(\"üß™ Or test with a single message:\")\n",
    "\n",
    "# Test with a single message\n",
    "test_history = [{\"role\": \"user\", \"parts\": [{\"text\": \"I'm starting a new microservices project. Based on our previous discussions, what architecture would you recommend?\"}]}]\n",
    "final_history = chat_with_memory_codebase_agent(test_history)\n",
    "\n",
    "print(\"\\n‚úÖ Demo completed! The assistant remembered the user's preferences for FastAPI, modular organization, and code quality standards.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cf81f8",
   "metadata": {},
   "source": [
    "## üéâ Conclusion\n",
    "\n",
    "Congratulations! You've successfully integrated mem0 (long-term memory) with Google Gemini into your codebase agent. Here's what we accomplished:\n",
    "\n",
    "1. **Memory Storage**: Stored user conversations and preferences \n",
    "2. **Context Retrieval**: Retrieved relevant memories for enhanced responses\n",
    "3. **Semantic Search**: Retrieved relevant memories based on context and meaning\n",
    "4. **Enhanced Responses**: Generated personalized responses using memory context\n",
    "5. **Interactive Chat**: Created a chatbot that remembers user preferences and project history\n",
    "\n",
    "### üîß Integration with Your Codebase Agent:\n",
    "\n",
    "Your project now includes:\n",
    "- `memory_manager.py` - Core memory functionality\n",
    "- `conversation_manager.py` - Enhanced with memory capabilities  \n",
    "- Updated `main.py` - New memory-enabled API endpoints\n",
    "\n",
    "### üöÄ New API Endpoints Available:\n",
    "- `/chat-with-memory` - Memory-aware conversations\n",
    "- `/analyze-repo-with-memory` - Repository analysis with user context\n",
    "- `/suggest-feature-with-memory` - Feature suggestions based on preferences\n",
    "- `/memory-search` - Search through user memories\n",
    "\n",
    "### üí° Benefits for Your Users:\n",
    "- **Personalized Experience**: Recommendations based on past interactions\n",
    "- **Context Continuity**: Maintains project context across sessions\n",
    "- **Learning System**: Gets better at understanding user preferences over time\n",
    "- **Repository-Specific Memory**: Different contexts for different projects\n",
    "\n",
    "### üîÆ Next Steps:\n",
    "1. Set your `GEMINI_API_KEY` environment variable\n",
    "2. Install the updated requirements: `pip install -r requirements.txt`\n",
    "3. Start your FastAPI server and test the new memory endpoints\n",
    "4. Experiment with the interactive chat functionality\n",
    "\n",
    "The memory system will make your codebase agent much more intelligent and user-friendly!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
